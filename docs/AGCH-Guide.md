# AGCH 無監督跨模態檢索架構：實驗重現與開發指南

1. 項目背景與 AGCH 技術戰略分析

在多媒體數據爆發的工業 4.0 時代，跨模態檢索（如以圖搜文、以文搜圖）已成為智慧搜尋系統的核心。然而，無監督學習場景因缺乏人工標註，長期面臨語義導引不足與模態異質性（Heterogeneity Gap）難以橋接的瓶頸。傳統基線如 DJSRH 或 UDCMH 多依賴單一相似度指標（通常為餘弦相似度），這在處理複雜數據關係時容易產生偏誤。

AGCH (Aggregation-based Graph Convolutional Hashing) 的戰略優勢在於突破了單一指標的侷限。其核心邏輯是透過「聚合相似度（Aggregation-based Similarity）」機制，同時捕捉數據的方向相關性（Orientation）維度量值差異（Difference），為模型提供具備穩健性的親和信號。

AGCH 的三大技術支柱

* 聚合相似度矩陣構建： 透過 Hadamard 乘積融合多重度量指標，生成更精確的親和矩陣（Affinity Matrix）。
* 多模態深度特徵編碼： 結合 CNN 與 MLP 提取非線性特徵，並透過融合模塊（Fusion Module）提取模態不變表徵。
* 圖卷積神經網絡（GCN）集成： 首次將 GCN 引入無監督雜湊，利用鄰域聚合機制（Neighborhood Aggregation）深度挖掘數據的內在流形結構。

架構戰略重點： 本架構的計算複雜度為 O(n)，具備優異的可擴展性，能有效處理大規模工業級數據集，是兼具學術深度與落地可行性的技術方案。

--------------------------------------------------------------------------------

1. 模型架構與組件規格說明

AGCH 採用端到端（End-to-End）框架，旨在同步優化特徵提取與雜湊碼生成。系統由模態特定編碼器、GCN 結構學習單元與跨模態融合模塊組成。

網絡組件規格表

組件名稱 Back-bone / 結構規格 關鍵參數與特徵層級 激活函數
影像編碼器 AlexNet (Pre-trained) 提取 fc-7 層特徵，接 c 維全連接層 ReLU, tanh (末層)
文本編碼器 三層 MLP K \to 4096 \to c (K 維度見第五章) ReLU, tanh (末層)
GCN 模塊 2 層圖卷積 + 1 層 FC 4096 \to 2048 \to c ReLU, tanh (末層)
融合模塊 全連接層 拼接 f^v 與 f^t 提取 B^h tanh

核心設計意圖

1. 特定模態編碼器： 產出 c 維連續向量 f^v 與 f^t，作為後續雜湊碼的基礎。
2. 融合模塊（Fusion Module）： 透過 B^h = \tanh(f(o^h; \theta_h)) 學習統一雜湊表徵，其作用是作為不同模態間的橋樑，過濾掉單一模態特有的噪音，保留共有的語義資訊。
3. GCN 模塊： 將聚合相似度矩陣作為鄰接矩陣，透過特徵傳播機制強化局部空間結構的表達。

--------------------------------------------------------------------------------

1. 聚合相似度矩陣（Similarity Matrix）構建邏輯

在無監督學習中，單一餘弦相似度僅衡量向量夾角，無法區分模態間的量值差異。AGCH 透過 Hadamard 乘積（Element-wise product）結合兩種指標，以獲得「低偏誤」的監督信號。

數學流程與公式重現

實作者必須嚴格遵循以下步驟，且**正規化（Normalization）**是確保 Hadamard 乘積有效的關鍵前提：

1. 特徵預處理： 對原始影像特徵 Z^v (AlexNet) 與文本特徵 Z^t (BoW/Tag) 進行正規化處理，並賦予權重 \gamma_v, \gamma_t： \tilde{Z}_{i*} = [\gamma_v\tilde{Z}^v_{i*}, \gamma_t\tilde{Z}^t_{i*}]
2. 方向相似度矩陣 (C_{ij})： 計算餘弦距離，捕捉向量方向相關性： C_{ij} = (\tilde{Z}_{i*})^T \tilde{Z}_{j*}
3. 差異相似度矩陣 (D_{ij})： 使用歐幾里得距離補償夾角指標的不足： D_{ij} = \exp\left(-\frac{\sqrt{\|\tilde{Z}_{i*} - \tilde{Z}_{j*}\|_2}}{\rho}\right)
4. 聚合與量化： 使用 Hadamard 乘積生成親和矩陣 S_{ij} = C_{ij} \cdot D_{ij}，並執行 S = 2S - 1 以調節量化範圍。

GCN 傳播規則

構建鄰接矩陣 \tilde{A}（基於 S）後，GCN 模塊必須遵循以下傳播公式進行特徵演化： H^{(l)} = \sigma(\tilde{D}^{-1/2}\tilde{A}\tilde{D}^{-1/2}H^{(l-1)}W^{(l)}) 其中 \tilde{D} 為度矩陣（Degree Matrix），此舉能確保在無標籤情況下，模型能透過鄰域節點的資訊交互來學習更高質量的語義表徵。

--------------------------------------------------------------------------------

1. 關鍵損失函數與優化策略

AGCH 透過多重損失函數實現模態內（Intra-modal）一致性與模態間（Inter-modal）對齊。

損失項分解

* L_1（相似度重構損失）： 強制學到的雜湊碼 B 在漢明空間中重構出 S。
* L_2（GCN 結構損失）： 確保 GCN 的輸出 B^g 保留鄰域結構，並與編碼器輸出 B 保持對齊。
* L_3（跨模態一致性損失）： 使 B^v, B^t 向融合碼 B^h 靠攏，縮減模態間隙。

優化算法：交替更新策略 (Alternating Update)

實作者應固定一組參數，更新另一組，避免梯度不穩定。

算法 1：AGCH 交替優化流程
輸入：訓練數據 O，雜湊長度 c，超參數 alpha, beta, lambda, rho, delta
過程：

1. 隨機初始化所有網絡參數 theta_k (k = v, t, h, gv, gt)。
2. 重複迭代直至收斂：
   a. 隨機選取 mini-batch。
   b. 提取特徵 Z^v, Z^t 並根據公式 (12) 構建聚合相似度矩陣 S。
   c. 固定其他參數，透過反向傳播更新單一模態編碼器 theta_v 或 theta_t。
   d. 計算融合表徵 o_h = f^v ⊕ f^t 並更新融合模塊 theta_h。
   e. 執行 GCN 前向傳播，計算損失 L = alpha*L1 + delta*L2 + L3。
   f. 利用 tanh 替代 sign 函數進行梯度回傳，更新 theta_gv 與 theta_gt。
返回：訓練完成之網絡。

--------------------------------------------------------------------------------

1. 實驗規劃與資料集預處理手冊

資料集的特徵類型決定了文本編碼器的輸入維度 K，實作者需特別注意 NUS-WIDE 的處理方式。

資料集技術規格對照表

資料集名稱 總樣本數 標籤類別 文本特徵類型 (K 維度) 影像特徵提取層
Wiki 2,866 10 10-d (LDA) AlexNet fc-7
MIRFlickr-25K 25,000 24 1386-d (PCA on BoW) AlexNet fc-7
NUS-WIDE 186,577 10 1000-d (Tag Index) AlexNet fc-7

超參數配置清單

針對不同規模的資料集，必須調整超參數以達到最佳效能：

資料集 \alpha \beta \lambda \gamma_v \gamma_t \rho
Wiki 0.4 0.3 5 1.0 0.8 4
MIRFlickr 1.0 1.0 10 2.0 0.3 4
NUS-WIDE 1.0 1.0 5 2.0 0.3 4

--------------------------------------------------------------------------------

1. 實驗結果驗證與檢查表

效能預期基準 (Golden Targets)

在重現過程中，應以此基準值驗證模型正確性。以 MIRFlickr-25K 為例，在 16 bits 設定下：

* I \to T (Image search Text): mAP 應達到 0.865。
* T \to I (Text search Image): mAP 應達到 0.829。
* 若數據顯著低於此數值，請優先檢查特徵正規化與 Hadamard 乘積 的實作細節。

消融實驗檢查 (Ablation Study Check)

透過移除組件來驗證各部分的貢獻度：

* AGCH-1 / AGCH-2： 僅用 Cosine 或 Euclidean，效能將大幅下降（驗證聚合的必要性）。
* AGCH-3 (Hybrid)： 文本用 Cosine、影像用 Euclidean。實驗證明此傳統組合效能仍低於 AGCH 的元素級 Hadamard 聚合。
* AGCH-4： 移除融合模塊（Fusion Module），觀察是否導致 bits 增加時效能增長受限。
* AGCH-5： 移除 GCN 模塊，驗證鄰域結構對雜湊碼判別力的影響。

收斂性診斷

* 預期收斂輪數： MIRFlickr 約 40 輪，Wiki 約 50-150 輪。
* 複雜度優勢： 訓練時間應不隨雜湊碼長度（bits）增加而有顯著增長，這證明了 O(n) 架構的工業價值。

結語： AGCH 透過精確的相似度聚合與圖卷積特徵挖掘，在無監督跨模態檢索中樹立了新的標竿。本手冊旨在引導工程師實現該模型，並建議在完成基礎重現後，進一步嘗試擴展至影音（Audio/Video）等多模態領域。
